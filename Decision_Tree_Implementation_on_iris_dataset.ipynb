{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision tree implementation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries and defining a binary class decision tree with both true and false branch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import collections\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "\n",
    "class DecisionTree:\n",
    "    #Binary tree implementation with true and false branch.\n",
    "    def __init__(self, col=-1, value=None, trueBranch=None, falseBranch=None, results=None):\n",
    "        self.col = col\n",
    "        self.value = value\n",
    "        self.trueBranch = trueBranch\n",
    "        self.falseBranch = falseBranch\n",
    "        self.results = results\n",
    "        \n",
    "       \n",
    "        \n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined a function shuffle which is used to shuffle the data to 70 % training and 30% testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def shuffle(data):\n",
    "    np.random.seed(2)\n",
    "    np.random.shuffle(data)\n",
    "    train_data = data[:int(0.7*len(data))]\n",
    "    test_data = data[int(0.7*len(data)):]\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined the divideset function to split the dataset and returning them to two lists "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def divideSet(rows, column, value):\n",
    "    splittingFunction = None\n",
    "    if isinstance(value, int) or isinstance(value, float): # for int and float values\n",
    "        splittingFunction = lambda row : row[column] >= value\n",
    "    else: # for strings \n",
    "        splittingFunction = lambda row : row[column] == value\n",
    "    list1 = [row for row in rows if splittingFunction(row)]\n",
    "    list2 = [row for row in rows if not splittingFunction(row)]\n",
    "    return (list1, list2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def uniqueCounts(rows):\n",
    "    results = {}\n",
    "    for row in rows:\n",
    "        r = row[-1]\n",
    "        if r not in results: results[r] = 0\n",
    "        results[r] += 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined the entropy splitting criterion function which calculates the information gain value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def entropy(rows):\n",
    "    from math import log\n",
    "    log2 = lambda x: log(x)/log(2)\n",
    "    results = uniqueCounts(rows)\n",
    "\n",
    "    entr = 0.0\n",
    "    for r in results:\n",
    "        p = float(results[r])/len(rows)\n",
    "        entr -= p*log2(p)\n",
    "      \n",
    "    return entr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined the gini splitting criterion function which calculates the gini index value "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def gini(rows):\n",
    "    total = len(rows)\n",
    "    counts = uniqueCounts(rows)\n",
    "    imp = 0.0\n",
    "\n",
    "    for k1 in counts:\n",
    "        p1 = float(counts[k1])/total  \n",
    "        for k2 in counts:\n",
    "            if k1 == k2: continue\n",
    "            p2 = float(counts[k2])/total\n",
    "            imp += p1*p2\n",
    "    \n",
    "    return imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def variance(rows):\n",
    "    if len(rows) == 0: return 0\n",
    "    data = [float(row[len(row) - 1]) for row in rows]\n",
    "    mean = sum(data) / len(data)\n",
    "\n",
    "    variance = sum([(d-mean)**2 for d in data]) / len(data)\n",
    "    return variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defined growDecisionTreeFrom function which is used to grow the binary decision tree using the evaluation function as entropy.\n",
    "Here we calculate the entropy gain and compare that with all splitting combinations and the best information gain is chose based on which the true and false branch are returned to the decision tree class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def growDecisionTreeFrom(rows, evaluationFunction=entropy):\n",
    "    \"\"\"Grows and then returns a binary decision tree.\n",
    "    evaluationFunction: entropy or gini\"\"\" \n",
    "    if len(rows) == 0: return DecisionTree()\n",
    "    currentScore = evaluationFunction(rows)\n",
    "    bestGain = 0.0\n",
    "    bestAttribute = None\n",
    "    bestSets = None\n",
    "    columnCount = len(rows[0]) - 1  # last column is the result/target column\n",
    "    for col in range(0, columnCount):\n",
    "        columnValues = [row[col] for row in rows]\n",
    "        for value in columnValues:\n",
    "            (set1, set2) = divideSet(rows, col, value)\n",
    "            p = float(len(set1)) / len(rows)\n",
    "            gain = currentScore - p*evaluationFunction(set1) - (1-p)*evaluationFunction(set2)\n",
    "            if gain>bestGain and len(set1)>0 and len(set2)>0:\n",
    "                bestGain = gain\n",
    "                bestAttribute = (col, value)\n",
    "                bestSets = (set1, set2)\n",
    "    if bestGain > 0:\n",
    "        trueBranch = growDecisionTreeFrom(bestSets[0])\n",
    "        falseBranch = growDecisionTreeFrom(bestSets[1])\n",
    "        \n",
    "        return DecisionTree(col=bestAttribute[0], value=bestAttribute[1], trueBranch=trueBranch, falseBranch=falseBranch)\n",
    "    else:\n",
    "        return DecisionTree(results=uniqueCounts(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defined growDecisionTreeFrom_gini function which is used to grow the binary decision tree using the evaluation function as gini.Here we calculate the gini index and compare that with all splitting combinations and the best gini gain is chose based on which the true and false branch are returned to the decision tree class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def growDecisionTreeFrom_gini(rows, evaluationFunction=gini):\n",
    "    \"\"\"Grows and then returns a binary decision tree.\n",
    "    evaluationFunction: entropy or gini\"\"\" \n",
    "    if len(rows) == 0: return DecisionTree()\n",
    "    currentScore = evaluationFunction(rows)\n",
    "    bestGain = 0.0\n",
    "    bestAttribute = None\n",
    "    bestSets = None\n",
    "    columnCount = len(rows[0]) - 1  # last column is the result/target column\n",
    "    for col in range(0, columnCount):\n",
    "        columnValues = [row[col] for row in rows]\n",
    "        for value in columnValues:\n",
    "            (set1, set2) = divideSet(rows, col, value)\n",
    "            p = float(len(set1)) / len(rows)\n",
    "            gain = currentScore - p*evaluationFunction(set1) - (1-p)*evaluationFunction(set2)\n",
    "            if gain>bestGain and len(set1)>0 and len(set2)>0:\n",
    "                bestGain = gain\n",
    "                bestAttribute = (col, value)\n",
    "                bestSets = (set1, set2)\n",
    "    if bestGain > 0:\n",
    "        trueBranch = growDecisionTreeFrom_gini(bestSets[0])\n",
    "        falseBranch = growDecisionTreeFrom_gini(bestSets[1])\n",
    "       \n",
    "       \n",
    "        return DecisionTree(col=bestAttribute[0], value=bestAttribute[1], trueBranch=trueBranch, falseBranch=falseBranch)\n",
    "    else:\n",
    "        return DecisionTree(results=uniqueCounts(rows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defined prune function which using mini gain as the parameter using which it prunes the decision tree based on the evaluation function as entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune(tree, minGain, evaluationFunction=entropy, notify=True):\n",
    "    \"\"\"Prunes the obtained tree according to the minimal gain (entropy or Gini). \"\"\"\n",
    "    # recursive call for each branch\n",
    "    if tree.trueBranch.results == None: prune(tree.trueBranch, minGain, evaluationFunction, notify)\n",
    "    if tree.falseBranch.results == None: prune(tree.falseBranch, minGain, evaluationFunction, notify)\n",
    "     # merge leaves (potentionally)\n",
    "    if tree.trueBranch.results != None and tree.falseBranch.results != None:\n",
    "        tb, fb = [], []\n",
    "        for v, c in tree.trueBranch.results.items(): tb += [[v]] * c\n",
    "        for v, c in tree.falseBranch.results.items(): fb += [[v]] * c\n",
    "        p = float(len(tb)) / len(tb + fb)\n",
    "        delta = evaluationFunction(tb+fb) - p*evaluationFunction(tb) - (1-p)*evaluationFunction(fb)\n",
    "        if delta < minGain:\t\n",
    "            if notify: print('A branch was pruned: gain = %f' % delta)\n",
    "            tree.trueBranch, tree.falseBranch = None, None\n",
    "            tree.results = uniqueCounts(tb + fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "defined prunegini function which using mini gain as the parameter using which it prunes the decision tree based on the evaluation function as gini."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def prune_gini(tree, minGain, evaluationFunction=gini, notify=True):\n",
    "    \"\"\"Prunes the obtained tree according to the minimal gain (entropy or Gini). \"\"\"\n",
    "    # recursive call for each branch\n",
    "    if tree.trueBranch.results == None: prune_gini(tree.trueBranch, minGain, evaluationFunction, notify)\n",
    "    if tree.falseBranch.results == None: prune_gini(tree.falseBranch, minGain, evaluationFunction, notify)\n",
    "     # merge leaves (potentionally)\n",
    "    if tree.trueBranch.results != None and tree.falseBranch.results != None:\n",
    "        tb, fb = [], []\n",
    "        for v, c in tree.trueBranch.results.items(): tb += [[v]] * c\n",
    "        for v, c in tree.falseBranch.results.items(): fb += [[v]] * c\n",
    "        p = float(len(tb)) / len(tb + fb)\n",
    "        delta = evaluationFunction(tb+fb) - p*evaluationFunction(tb) - (1-p)*evaluationFunction(fb)\n",
    "        if delta < minGain:\t\n",
    "            if notify: print('A branch was pruned: gain = %f' % delta)\n",
    "            tree.trueBranch, tree.falseBranch = None, None\n",
    "            tree.results = uniqueCounts(tb + fb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function classify is used to classify the test data set with the training tree and observes if easch data in the data set is classified in the right way or not and returns the observation along  with the branch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(observations, tree, dataMissing=False):\n",
    "    \"\"\"Classifies the observationss according to the tree.\n",
    "     dataMissing: true or false if data are missing or not. \"\"\"\n",
    "\n",
    "    def classifyWithoutMissingData(observations, tree):\n",
    "        if tree.results != None:  # leaf\n",
    "            return observations[4]\n",
    "        else:\n",
    "            v = observations[tree.col]\n",
    "            branch = None\n",
    "            if isinstance(v, int) or isinstance(v, float):\n",
    "                if v >= tree.value: branch = tree.trueBranch\n",
    "                else: branch = tree.falseBranch\n",
    "            else:\n",
    "                if v == tree.value: branch = tree.trueBranch\n",
    "                else: branch = tree.falseBranch\n",
    "        return classifyWithoutMissingData(observations,branch)\n",
    "\n",
    "\n",
    "    def classifyWithMissingData(observations, tree):\n",
    "        if tree.results != None:  # leaf \n",
    "            return observations[4]\n",
    "        else:\n",
    "            v = observations[tree.col]\n",
    "            if v == None:\n",
    "                tr = classifyWithMissingData(observations, tree.trueBranch)\n",
    "                fr = classifyWithMissingData(observations, tree.falseBranch)\n",
    "                tcount = sum(tr.values())\n",
    "                fcount = sum(fr.values())\n",
    "                tw = float(tcount)/(tcount + fcount)\n",
    "                fw = float(fcount)/(tcount + fcount)\n",
    "                result = collections.defaultdict(int) # Problem description: http://blog.ludovf.net/python-collections-defaultdict/\n",
    "                for k, v in tr.items(): result[k] += v*tw\n",
    "                for k, v in fr.items(): result[k] += v*fw\n",
    "                return dict(result)\n",
    "            else:\n",
    "                branch = None\n",
    "                if isinstance(v, int) or isinstance(v, float):\n",
    "                    if v >= tree.value: branch = tree.trueBranch\n",
    "                    else: branch = tree.falseBranch\n",
    "                else:\n",
    "                    if v == tree.value: branch = tree.trueBranch\n",
    "                    else: branch = tree.falseBranch\n",
    "            return classifyWithMissingData(observations, branch)\n",
    "\n",
    "    # function body\n",
    "    if dataMissing: \n",
    "        return classifyWithMissingData(observations, tree)\n",
    "    else:\n",
    "        return classifyWithoutMissingData(observations, tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defined plot function to plot the training dataset set in the form of a binary tree. the best gain is considered as the root node and based on which we classify the true and false branch and such binary trees of many depths are constructed in the similar way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(decisionTree):\n",
    "    \"\"\"Plots the obtained decision tree. \"\"\"\n",
    "    def toString(decisionTree, indent=''):\n",
    "        if decisionTree.results != None:  # leaf node\n",
    "            return str(decisionTree.results)\n",
    "        else:\n",
    "            szCol = 'Column %s' % decisionTree.col\n",
    "            if szCol in dcHeadings:\n",
    "                szCol = dcHeadings[szCol]\n",
    "            if isinstance(decisionTree.value, int) or isinstance(decisionTree.value, float):\n",
    "                decision = '%s >= %s?' % (szCol, decisionTree.value)\n",
    "            else:\n",
    "                decision = '%s == %s?' % (szCol, decisionTree.value)\n",
    "            trueBranch = indent + 'yes -> ' + toString(decisionTree.trueBranch, indent + '\\t\\t')\n",
    "            falseBranch = indent + 'no  -> ' + toString(decisionTree.falseBranch, indent + '\\t\\t')\n",
    "            \n",
    "            return (decision + '\\n' + trueBranch + '\\n' + falseBranch)\n",
    "\n",
    "    print(toString(decisionTree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def loadCSV(file):\n",
    "    \"\"\"Loads a CSV file and converts all floats and int into basic datatypes.\"\"\" \n",
    "    def convertTypes(s):\n",
    "        s = s.strip()\n",
    "        try:\n",
    "            return float(s) if '.' in s else int(s)\n",
    "        except ValueError:\n",
    "            return s\n",
    "    reader = csv.reader(open(file, 'rt'))\n",
    "    dcHeader = {}\n",
    "    if bHeader:\n",
    "        lsHeader = next(reader)\n",
    "        for i, szY in enumerate(lsHeader):\n",
    "                szCol = 'Column %d' % i\n",
    "                dcHeader[szCol] = str(szY)\n",
    "    return dcHeader, [[convertTypes(item) for item in row] for row in reader]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the iris dataset and plotting the training data set to a binary tree using evaluation function as entropy and calculated the misclassification rate.Pruning results are also calculated on entropy tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "45\n",
      "PetalLength >= 3?\n",
      "yes -> PetalLength >= 4.8?\n",
      "\t\tyes -> PetalWidth >= 1.8?\n",
      "\t\t\t\tyes -> {'virginica': 33}\n",
      "\t\t\t\tno  -> PetalWidth >= 1.7?\n",
      "\t\t\t\t\t\tyes -> {'versicolor': 1}\n",
      "\t\t\t\t\t\tno  -> {'virginica': 3}\n",
      "\t\tno  -> PetalWidth >= 1.7?\n",
      "\t\t\t\tyes -> {'virginica': 1}\n",
      "\t\t\t\tno  -> {'versicolor': 30}\n",
      "no  -> {'setosa': 37}\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "test data before classification: ['setosa' 'setosa' 'versicolor' 'virginica' 'versicolor' 'virginica'\n",
      " 'versicolor' 'versicolor' 'virginica' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'versicolor' 'virginica' 'versicolor' 'setosa'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'virginica' 'versicolor' 'virginica'\n",
      " 'setosa' 'virginica' 'setosa' 'virginica' 'virginica' 'setosa'\n",
      " 'versicolor' 'setosa' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa']\n",
      "test_data after classification: ['setosa' 'setosa' 'versicolor' 'virginica' 'versicolor' 'virginica'\n",
      " 'versicolor' 'versicolor' 'virginica' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'versicolor' 'virginica' 'versicolor' 'setosa'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'virginica' 'versicolor' 'virginica'\n",
      " 'setosa' 'virginica' 'setosa' 'virginica' 'virginica' 'setosa'\n",
      " 'versicolor' 'setosa' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa']\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "misclassification rate 0\n",
      "A branch was pruned: gain = 0.811278\n",
      "A branch was pruned: gain = 0.091550\n",
      "A branch was pruned: gain = 0.205593\n",
      "A branch was pruned: gain = 0.803114\n",
      "PetalLength >= 3?\n",
      "yes -> {'virginica': 37, 'versicolor': 31}\n",
      "no  -> {'setosa': 37}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    example = 1\n",
    "    if example == 1:\n",
    "        bHeader = True\n",
    "        count = 0\n",
    "        classified_resultlist=[]\n",
    "        classified_resultlist_gini=[]\n",
    "        \n",
    "        # the bigger example\n",
    "        dcHeadings,Data = loadCSV('C:\\Users\\saikiran\\Desktop\\iris.txt') # demo data from matlab\n",
    "        train_dataset, test_dataset = shuffle(Data)\n",
    "        print(len(train_dataset))\n",
    "        print(len(test_dataset))\n",
    "        decisionTree = growDecisionTreeFrom(train_dataset)\n",
    "       \n",
    "        plot(decisionTree)\n",
    "       \n",
    "        \n",
    "        test = np.asarray(test_dataset)\n",
    "        result1 = test[:,4]\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        print \"test data before classification:\", result1\n",
    "        for i in test_dataset:\n",
    "            classified_result= classify(i, decisionTree) # dataMissing=False is the default setting\n",
    "            classified_resultlist = np.append(classified_resultlist,classified_result)\n",
    "       \n",
    "        print \"test_data after classification:\", classified_resultlist\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        print \"misclassification rate\", count\n",
    "        \n",
    "        # notify, when a branch is pruned \n",
    "        prune(decisionTree, 0.9, notify=True) \n",
    "        plot(decisionTree)\n",
    "        \n",
    "       \n",
    "       \n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the iris dataset and plotting the training data set to a binary tree using evaluation function as gini and calculated the misclassification rate. Pruning results are also calculated on gini tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n",
      "45\n",
      "PetalLength >= 3?\n",
      "yes -> PetalLength >= 4.8?\n",
      "\t\tyes -> PetalWidth >= 1.8?\n",
      "\t\t\t\tyes -> {'virginica': 33}\n",
      "\t\t\t\tno  -> PetalWidth >= 1.7?\n",
      "\t\t\t\t\t\tyes -> {'versicolor': 1}\n",
      "\t\t\t\t\t\tno  -> {'virginica': 3}\n",
      "\t\tno  -> PetalWidth >= 1.7?\n",
      "\t\t\t\tyes -> {'virginica': 1}\n",
      "\t\t\t\tno  -> {'versicolor': 30}\n",
      "no  -> {'setosa': 37}\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "test data before classification: ['setosa' 'setosa' 'versicolor' 'virginica' 'versicolor' 'virginica'\n",
      " 'versicolor' 'versicolor' 'virginica' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'versicolor' 'virginica' 'versicolor' 'setosa'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'virginica' 'versicolor' 'virginica'\n",
      " 'setosa' 'virginica' 'setosa' 'virginica' 'virginica' 'setosa'\n",
      " 'versicolor' 'setosa' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa']\n",
      "test_data after classification: ['setosa' 'setosa' 'versicolor' 'virginica' 'versicolor' 'virginica'\n",
      " 'versicolor' 'versicolor' 'virginica' 'versicolor' 'versicolor'\n",
      " 'versicolor' 'virginica' 'versicolor' 'virginica' 'versicolor' 'setosa'\n",
      " 'versicolor' 'versicolor' 'versicolor' 'versicolor' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'virginica' 'versicolor' 'virginica'\n",
      " 'setosa' 'virginica' 'setosa' 'virginica' 'virginica' 'setosa'\n",
      " 'versicolor' 'setosa' 'virginica' 'versicolor' 'setosa' 'virginica'\n",
      " 'versicolor' 'setosa' 'setosa' 'versicolor' 'setosa']\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "misclassification rate 0\n",
      "--------------------------------------------------------------------------------------------------------\n",
      "A branch was pruned: gain = 0.375000\n",
      "A branch was pruned: gain = 0.012053\n",
      "A branch was pruned: gain = 0.062435\n",
      "PetalLength >= 3?\n",
      "yes -> PetalLength >= 4.8?\n",
      "\t\tyes -> {'virginica': 36, 'versicolor': 1}\n",
      "\t\tno  -> {'virginica': 1, 'versicolor': 30}\n",
      "no  -> {'setosa': 37}\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "\n",
    "    example = 2\n",
    "    if example == 2:\n",
    "        bHeader = True\n",
    "        count = 0\n",
    "        \n",
    "        classified_resultlist_gini=[]\n",
    "        \n",
    "        # the bigger example\n",
    "        dcHeadings,Data = loadCSV('C:\\Users\\saikiran\\Desktop\\iris.txt') # demo data from matlab\n",
    "        train_dataset, test_dataset = shuffle(Data)\n",
    "        print(len(train_dataset))\n",
    "        print(len(test_dataset))\n",
    "       \n",
    "        test = np.asarray(test_dataset)\n",
    "        result1 = test[:,4]\n",
    "       \n",
    "        decisionTree_gini = growDecisionTreeFrom_gini(train_dataset)\n",
    "        plot(decisionTree_gini)\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        print \"test data before classification:\", result1\n",
    "        for j in test_dataset:\n",
    "            classified_result_gini= classify(j, decisionTree_gini) # dataMissing=False is the default setting\n",
    "            classified_resultlist_gini = np.append(classified_resultlist_gini,classified_result_gini)\n",
    "        print \"test_data after classification:\", classified_resultlist_gini\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        print \"misclassification rate\", count\n",
    "        print(\"--------------------------------------------------------------------------------------------------------\")\n",
    "        prune_gini(decisionTree_gini, 0.4, notify=True) \n",
    "        plot(decisionTree_gini)\n",
    "        \n",
    "      \n",
    "        \n",
    "        \n",
    "\n",
    "            \n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
